{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业五：机器翻译\n",
    "使用seq2seq模型完成英语-->繁体中文的翻译任务，数据集来自TED演讲，使用BLEU分数来评价模型\n",
    "训练技巧：\n",
    "- Tokenize data with sub-word units：子词汇切分 使用子字单元标记数据\n",
    "- Label smoothing regularization：标签平滑化 计算loss时对不正确的标签也保留一些概率，这样可以避免过拟合\n",
    "- Learning rate scheduling：学习率调整 采用warm-up机制\n",
    "- Back translation：反向翻译 比如当前的这个任务，可以再训一个中文到英语的任务，然后拿中文单语言词库反向生成英语词汇，进而作为模型训练的补充资料"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安装必要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install editdistance sacrebleu sacremoses sentencepiece tqdm wandb\n",
    "# %pip install --upgrade jupyter ipywidgets\n",
    "# !git clone https://github.com/pytorch/fairseq.git\n",
    "# !cd fairseq && git checkout 9a1c497\n",
    "# %pip install --upgrade ./fairseq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pdb\n",
    "import pprint\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import tqdm.auto as tqdm\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "from fairseq import utils\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 73\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "np.random.seed(seed)  \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载数据集\n",
    "\n",
    "双语言数据库：TED2020\n",
    "\n",
    "Raw: 398,066 (sentences)\n",
    "\n",
    "Processed: 393,980 (sentences)\n",
    "\n",
    "测试集：Size: 4,000 (sentences)\n",
    "\n",
    "中文翻译未公开。提供的（.zh）文件是伪翻译，每一行都是一个'。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw.en\n",
      "raw.zh\n",
      "test/\n",
      "test/test.zh\n",
      "test/test.en\n"
     ]
    }
   ],
   "source": [
    "data_dir = './DATA/rawdata'\n",
    "dataset_name = 'ted2020'\n",
    "urls = (\n",
    "    \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/ted2020.tgz\",\n",
    "    \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/test.tgz\",\n",
    ")\n",
    "file_names = (\n",
    "    'ted2020.tgz', # train & dev\n",
    "    'test.tgz', # test\n",
    ")\n",
    "prefix = Path(data_dir).absolute() / dataset_name\n",
    "\n",
    "prefix.mkdir(parents=True, exist_ok=True)\n",
    "for u, f in zip(urls, file_names):\n",
    "    path = prefix/f\n",
    "    if not path.exists():\n",
    "        !wget {u} -O {path}\n",
    "    if path.suffix == \".tgz\":\n",
    "        !tar -xvf {path} -C {prefix}\n",
    "    elif path.suffix == \".zip\":\n",
    "        !unzip -o {path} -d {prefix}\n",
    "!mv {prefix/'raw.en'} {prefix/'train_dev.raw.en'}\n",
    "!mv {prefix/'raw.zh'} {prefix/'train_dev.raw.zh'}\n",
    "!mv {prefix/'test/test.en'} {prefix/'test.raw.en'}\n",
    "!mv {prefix/'test/test.zh'} {prefix/'test.raw.zh'}\n",
    "!rm -rf {prefix/'test'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language\n",
    "src_lang = 'en'\n",
    "tgt_lang = 'zh'\n",
    "\n",
    "data_prefix = f'{prefix}/train_dev.raw'\n",
    "test_prefix = f'{prefix}/test.raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you so much, Chris.\n",
      "And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
      "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
      "And I say that sincerely, partly because  I need that.\n",
      "Put yourselves in my position.\n",
      "非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台\n",
      "真是一大榮幸。我非常感激。\n",
      "這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。\n",
      "我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!\n",
      "請你們設身處地為我想一想！\n"
     ]
    }
   ],
   "source": [
    "!head {data_prefix+'.'+src_lang} -n 5\n",
    "!head {data_prefix+'.'+tgt_lang} -n 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def strQ2B(ustring):\n",
    "    \"\"\"Full width -> half width\"\"\"\n",
    "    # reference:https://ithelp.ithome.com.tw/articles/10233122\n",
    "    ss = []\n",
    "    for s in ustring:\n",
    "        rstring = \"\"\n",
    "        for uchar in s:\n",
    "            inside_code = ord(uchar)\n",
    "            if inside_code == 12288:  # Full width space: direct conversion\n",
    "                inside_code = 32\n",
    "            elif (inside_code >= 65281 and inside_code <= 65374):  # Full width chars (except space) conversion\n",
    "                inside_code -= 65248\n",
    "            rstring += chr(inside_code)\n",
    "        ss.append(rstring)\n",
    "    return ''.join(ss)\n",
    "                \n",
    "def clean_s(s, lang):\n",
    "    if lang == 'en':\n",
    "        s = re.sub(r\"【（）】*\", \"\", s) # remove ([text])\n",
    "        s = s.replace('-', '') # remove '-'\n",
    "        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) # keep punctuation\n",
    "    elif lang == 'zh':\n",
    "        s = strQ2B(s) # Q2B\n",
    "        s = re.sub(r\"【（）】*\", \"\", s) # remove ([text])\n",
    "        s = s.replace(' ', '')\n",
    "        s = s.replace('—', '')\n",
    "        s = s.replace('“', '\"')\n",
    "        s = s.replace('”', '\"')\n",
    "        s = s.replace('_', '')\n",
    "        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
    "    s = ' '.join(s.strip().split())\n",
    "    return s\n",
    "\n",
    "def len_s(s, lang):\n",
    "    if lang == 'zh':\n",
    "        return len(s)\n",
    "    return len(s.split())\n",
    "\n",
    "def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
    "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
    "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
    "        return\n",
    "    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
    "        with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n",
    "            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
    "                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
    "                    for s1 in l1_in_f:\n",
    "                        s1 = s1.strip()\n",
    "                        s2 = l2_in_f.readline().strip()\n",
    "                        s1 = clean_s(s1, l1)\n",
    "                        s2 = clean_s(s2, l2)\n",
    "                        s1_len = len_s(s1, l1)\n",
    "                        s2_len = len_s(s2, l2)\n",
    "                        if min_len > 0: # remove short sentence\n",
    "                            if s1_len < min_len or s2_len < min_len:\n",
    "                                continue\n",
    "                        if max_len > 0: # remove long sentence\n",
    "                            if s1_len > max_len or s2_len > max_len:\n",
    "                                continue\n",
    "                        if ratio > 0: # remove by ratio of length\n",
    "                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
    "                                continue\n",
    "                        print(s1, file=l1_out_f)\n",
    "                        print(s2, file=l2_out_f)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/DATA/rawdata/ted2020/train_dev.raw.clean.en & zh exists. skipping clean.\n",
      "/Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/DATA/rawdata/ted2020/test.raw.clean.en & zh exists. skipping clean.\n"
     ]
    }
   ],
   "source": [
    "clean_corpus(data_prefix, src_lang, tgt_lang)\n",
    "clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you so much , Chris .\n",
      "And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\n",
      "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
      "And I say that sincerely , partly because I need that .\n",
      "Put yourselves in my position .\n",
      "非常謝謝你 , 克里斯 。 能有這個機會第二度踏上這個演講台\n",
      "真是一大榮幸 。 我非常感激 。\n",
      "這個研討會給我留下了極為深刻的印象 , 我想感謝大家對我之前演講的好評 。\n",
      "我是由衷的想這麼說 , 有部份原因是因為我真的有需要 !\n",
      "請你們設身處地為我想一想 !\n"
     ]
    }
   ],
   "source": [
    "!head {data_prefix+'.clean.'+src_lang} -n 5\n",
    "!head {data_prefix+'.clean.'+tgt_lang} -n 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.01 # 3000~4000 would suffice\n",
    "train_ratio = 1 - valid_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/valid splits exists. skipping split.\n"
     ]
    }
   ],
   "source": [
    "if (prefix/f'train.clean.{src_lang}').exists() \\\n",
    "and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
    "and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
    "and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
    "    print(f'train/valid splits exists. skipping split.')\n",
    "else:\n",
    "    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
    "    labels = list(range(line_num))\n",
    "    random.shuffle(labels)\n",
    "    for lang in [src_lang, tgt_lang]:\n",
    "        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
    "        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
    "        count = 0\n",
    "        for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
    "            if labels[count]/line_num < train_ratio:\n",
    "                train_f.write(line)\n",
    "            else:\n",
    "                valid_f.write(line)\n",
    "            count += 1\n",
    "        train_f.close()\n",
    "        valid_f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 子词汇单元\n",
    "机器翻译中常出现未登录词 Out-of-vocabulary（OOV）的问题，就是训练时未出现，测试时出现了的单词。使用子词汇划分(subword units)的方法可以缓解这种情况\n",
    "\n",
    "我们使用sentencepiece库，使用 'unigram' 或 'byte-pair encoding (BPE)' 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/DATA/rawdata/ted2020/spm8000.model exists. skipping spm_train.\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "vocab_size = 8000\n",
    "if (prefix/f'spm{vocab_size}.model').exists():\n",
    "    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
    "else:\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
    "                        f'{prefix}/valid.clean.{src_lang}',\n",
    "                        f'{prefix}/train.clean.{tgt_lang}',\n",
    "                        f'{prefix}/valid.clean.{tgt_lang}']),\n",
    "        model_prefix=prefix/f'spm{vocab_size}',\n",
    "        vocab_size=vocab_size,\n",
    "        character_coverage=1,\n",
    "        model_type='unigram', # 'bpe' works as well\n",
    "        input_sentence_size=1e6,\n",
    "        shuffle_input_sentence=True,\n",
    "        normalization_rule_name='nmt_nfkc_cf',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/DATA/rawdata/ted2020/train.en exists. skipping spm_encode.\n",
      "/Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/DATA/rawdata/ted2020/train.zh exists. skipping spm_encode.\n",
      "/Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/DATA/rawdata/ted2020/valid.en exists. skipping spm_encode.\n",
      "/Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/DATA/rawdata/ted2020/valid.zh exists. skipping spm_encode.\n",
      "/Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/DATA/rawdata/ted2020/test.en exists. skipping spm_encode.\n",
      "/Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/DATA/rawdata/ted2020/test.zh exists. skipping spm_encode.\n"
     ]
    }
   ],
   "source": [
    "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
    "in_tag = {\n",
    "    'train': 'train.clean',\n",
    "    'valid': 'valid.clean',\n",
    "    'test': 'test.raw.clean',\n",
    "}\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    for lang in [src_lang, tgt_lang]:\n",
    "        out_path = prefix/f'{split}.{lang}'\n",
    "        if out_path.exists():\n",
    "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
    "        else:\n",
    "            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
    "                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
    "                    for line in in_f:\n",
    "                        line = line.strip()\n",
    "                        tok = spm_model.encode(line, out_type=str)\n",
    "                        print(' '.join(tok), file=out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁thank ▁you ▁so ▁much ▁, ▁chris ▁.\n",
      "▁and ▁it ' s ▁ t ru ly ▁a ▁great ▁ho n or ▁to ▁have ▁the ▁ op port un ity ▁to ▁come ▁to ▁this ▁st age ▁ t wi ce ▁; ▁i ' m ▁ex t re me ly ▁gr ate ful ▁.\n",
      "▁i ▁have ▁been ▁ bl ow n ▁away ▁by ▁this ▁con fer ence ▁, ▁and ▁i ▁want ▁to ▁thank ▁all ▁of ▁you ▁for ▁the ▁many ▁ ni ce ▁ com ment s ▁about ▁what ▁i ▁had ▁to ▁say ▁the ▁other ▁night ▁.\n",
      "▁and ▁i ▁say ▁that ▁since re ly ▁, ▁part ly ▁because ▁i ▁need ▁that ▁.\n",
      "▁put ▁your s el ve s ▁in ▁my ▁po s ition ▁.\n",
      "▁ 非常 謝 謝 你 ▁, ▁ 克 里 斯 ▁。 ▁ 能 有 這個 機會 第二 度 踏 上 這個 演講 台\n",
      "▁ 真 是 一 大 榮 幸 ▁。 ▁我 非常 感 激 ▁。\n",
      "▁這個 研 討 會 給我 留 下 了 極 為 深 刻 的 印 象 ▁, ▁我想 感 謝 大家 對我 之前 演講 的 好 評 ▁。\n",
      "▁我 是由 衷 的 想 這麼 說 ▁, ▁有 部份 原因 是因為 我 真的 有 需要 ▁!\n",
      "▁ 請 你們 設 身 處 地 為 我想 一 想 ▁!\n"
     ]
    }
   ],
   "source": [
    "!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
    "!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用fairseq库二值化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA/data-bin/ted2020 exists, will not overwrite!\n"
     ]
    }
   ],
   "source": [
    "binpath = Path('./DATA/data-bin', dataset_name)\n",
    "if binpath.exists():\n",
    "    print(binpath, \"exists, will not overwrite!\")\n",
    "else:\n",
    "    !python -m fairseq_cli.preprocess \\\n",
    "        --source-lang {src_lang}\\\n",
    "        --target-lang {tgt_lang}\\\n",
    "        --trainpref {prefix/'train'}\\\n",
    "        --validpref {prefix/'valid'}\\\n",
    "        --testpref {prefix/'test'}\\\n",
    "        --destdir {binpath}\\\n",
    "        --joined-dictionary\\\n",
    "        --workers 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Namespace(\n",
    "    datadir = \"./DATA/data-bin/ted2020\",\n",
    "    savedir = \"./checkpoints/transformer\",\n",
    "    source_lang = \"en\",\n",
    "    target_lang = \"zh\",\n",
    "    \n",
    "    # cpu threads when fetching & processing data.\n",
    "    num_workers=2,  \n",
    "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
    "    max_tokens=8192,\n",
    "    accum_steps=2,\n",
    "    \n",
    "    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
    "    lr_factor=2.,\n",
    "    lr_warmup=4000,\n",
    "    \n",
    "    # clipping gradient norm helps alleviate gradient exploding\n",
    "    clip_norm=1.0,\n",
    "    \n",
    "    # maximum epochs for training\n",
    "    max_epoch=40,\n",
    "    start_epoch=1,\n",
    "    \n",
    "    # beam size for beam search\n",
    "    beam=5, \n",
    "    # generate sequences of maximum length ax + b, where x is the source length\n",
    "    max_len_a=1.2, \n",
    "    max_len_b=10, \n",
    "    # when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.\n",
    "    post_process = \"sentencepiece\",\n",
    "    \n",
    "    # checkpoints\n",
    "    keep_last_epochs=5,\n",
    "    resume=None, # if resume from checkpoint name (under config.savedir)\n",
    "    \n",
    "    # logging\n",
    "    use_wandb=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 日志输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 10:37:14 | ERROR | wandb.jupyter | Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrichard_wang\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/wandb/run-20230120_103716-ogowtl83</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/richard_wang/hw5_transformer/runs/ogowtl83\" target=\"_blank\">transformer</a></strong> to <a href=\"https://wandb.ai/richard_wang/hw5_transformer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/richard_wang/hw5_transformer\" target=\"_blank\">https://wandb.ai/richard_wang/hw5_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/richard_wang/hw5_transformer/runs/ogowtl83\" target=\"_blank\">https://wandb.ai/richard_wang/hw5_transformer/runs/ogowtl83</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "proj = \"hw5_transformer\"\n",
    "logger = logging.getLogger(proj)\n",
    "if config.use_wandb:\n",
    "    import wandb\n",
    "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 10:37:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-01-20 10:37:18 | INFO | fairseq.utils | rank   0: capabilities =  5.2  ; total memory = 23.905 GB ; name = Tesla M40 24GB                          \n",
      "2023-01-20 10:37:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n"
     ]
    }
   ],
   "source": [
    "cuda_env = utils.CudaEnvironment()\n",
    "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集加载\n",
    "我们采用现成的功能强大的fairseq库来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 10:37:19 | INFO | fairseq.tasks.translation | [en] dictionary: 8000 types\n",
      "2023-01-20 10:37:19 | INFO | fairseq.tasks.translation | [zh] dictionary: 8000 types\n"
     ]
    }
   ],
   "source": [
    "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
    "\n",
    "## setup task\n",
    "task_cfg = TranslationConfig(\n",
    "    data=config.datadir,\n",
    "    source_lang=config.source_lang,\n",
    "    target_lang=config.target_lang,\n",
    "    train_subset=\"train\",\n",
    "    required_seq_len_multiple=8,\n",
    "    dataset_impl=\"mmap\",\n",
    "    upsample_primary=1,\n",
    ")\n",
    "task = TranslationTask.setup_task(task_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 10:37:19 | INFO | hw5_transformer | loading data for epoch 1\n",
      "2023-01-20 10:37:19 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020/train.en-zh.en\n",
      "2023-01-20 10:37:19 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020/train.en-zh.zh\n",
      "2023-01-20 10:37:19 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 train en-zh 390041 examples\n",
      "2023-01-20 10:37:19 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020/valid.en-zh.en\n",
      "2023-01-20 10:37:19 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020/valid.en-zh.zh\n",
      "2023-01-20 10:37:19 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 valid en-zh 3939 examples\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"loading data for epoch 1\")\n",
    "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
    "task.load_dataset(split=\"valid\", epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1,\n",
      " 'source': tensor([  18,   14,    6, 2234,   60,   19,   80,    5,  256,   16,  405, 1407,\n",
      "        1706,    7,    2]),\n",
      " 'target': tensor([ 140,  690,   28,  270,   45,  151, 1142,  660,  606,  369, 3114, 2434,\n",
      "        1434,  192,    2])}\n",
      "\"Source: that's exactly what i do optical mind control .\"\n",
      "'Target: 這實在就是我所做的--光學操控思想'\n"
     ]
    }
   ],
   "source": [
    "sample = task.dataset(\"valid\")[1]\n",
    "pprint.pprint(sample)\n",
    "pprint.pprint(\n",
    "    \"Source: \" + \\\n",
    "    task.source_dictionary.string(\n",
    "        sample['source'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")\n",
    "pprint.pprint(\n",
    "    \"Target: \" + \\\n",
    "    task.target_dictionary.string(\n",
    "        sample['target'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集迭代器\n",
    "\n",
    "- 控制每个批次包含不超过 N 个令牌，从而优化 GPU 内存效率\n",
    "- 为每个时期打乱训练集\n",
    "- 忽略超过最大长度的句子\n",
    "- 将一批中的所有句子填充到相同的长度，从而实现 GPU 的并行计算\n",
    "- 添加eos并转移一个令牌\n",
    "    - teacher forcing：为了训练模型根据前缀预测下一个标记，我们将右移的目标序列作为解码器输入。\n",
    "    - 通常，将 bos 添加到目标之前就可以完成这项工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 10:37:20 | WARNING | fairseq.tasks.fairseq_task | 2,532 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[29, 135, 2444, 3058, 682, 731, 235, 1558, 3383, 559]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': tensor([723]),\n",
       " 'nsentences': 1,\n",
       " 'ntokens': 18,\n",
       " 'net_input': {'src_tokens': tensor([[   1,    1,    1,    1,    1,   18,   26,   82,    8,  480,   15,  651,\n",
       "           1361,   38,    6,  176, 2696,   39,    5,  822,   92,  260,    7,    2]]),\n",
       "  'src_lengths': tensor([19]),\n",
       "  'prev_output_tokens': tensor([[   2,  140,  296,  318, 1560,   51,  568,  316,  225, 1952,  254,   78,\n",
       "            151, 2691,    9,  215, 1680,   10,    1,    1,    1,    1,    1,    1]])},\n",
       " 'target': tensor([[ 140,  296,  318, 1560,   51,  568,  316,  225, 1952,  254,   78,  151,\n",
       "          2691,    9,  215, 1680,   10,    2,    1,    1,    1,    1,    1,    1]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
    "    batch_iterator = task.get_batch_iterator(\n",
    "        dataset=task.dataset(split),\n",
    "        max_tokens=max_tokens,\n",
    "        max_sentences=None,\n",
    "        max_positions=utils.resolve_max_positions(\n",
    "            task.max_positions(),\n",
    "            max_tokens,\n",
    "        ),\n",
    "        ignore_invalid_inputs=True,\n",
    "        seed=seed,\n",
    "        num_workers=num_workers,\n",
    "        epoch=epoch,\n",
    "        disable_iterator_cache=not cached,\n",
    "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n",
    "        # first call of this method has no effect. \n",
    "    )\n",
    "    return batch_iterator\n",
    "\n",
    "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
    "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
    "sample = next(demo_iter)\n",
    "sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型架构\n",
    "这里直接采用Transformer架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models import FairseqEncoder, FairseqIncrementalDecoder,FairseqEncoderDecoderModel\n",
    "\n",
    "# Attention Layer\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_embed_dim, source_embed_dim, output_embed_dim, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = nn.Linear(input_embed_dim, source_embed_dim, bias=bias)\n",
    "        self.output_proj = nn.Linear(\n",
    "            input_embed_dim + source_embed_dim, output_embed_dim, bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, encoder_outputs, encoder_padding_mask):\n",
    "        # inputs: T, B, dim\n",
    "        # encoder_outputs: S x B x dim\n",
    "        # padding mask:  S x B\n",
    "        \n",
    "        # convert all to batch first\n",
    "        inputs = inputs.transpose(1,0) # B, T, dim\n",
    "        encoder_outputs = encoder_outputs.transpose(1,0) # B, S, dim\n",
    "        encoder_padding_mask = encoder_padding_mask.transpose(1,0) # B, S\n",
    "        \n",
    "        # project to the dimensionality of encoder_outputs\n",
    "        x = self.input_proj(inputs)\n",
    "\n",
    "        # compute attention\n",
    "        # (B, T, dim) x (B, dim, S) = (B, T, S)\n",
    "        attn_scores = torch.bmm(x, encoder_outputs.transpose(1,2))\n",
    "\n",
    "        # cancel the attention at positions corresponding to padding\n",
    "        if encoder_padding_mask is not None:\n",
    "            # leveraging broadcast  B, S -> (B, 1, S)\n",
    "            encoder_padding_mask = encoder_padding_mask.unsqueeze(1)\n",
    "            attn_scores = (\n",
    "                attn_scores.float()\n",
    "                .masked_fill_(encoder_padding_mask, float(\"-inf\"))\n",
    "                .type_as(attn_scores)\n",
    "            )  # FP16 support: cast to float and back\n",
    "\n",
    "        # softmax on the dimension corresponding to source sequence\n",
    "        attn_scores = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # shape (B, T, S) x (B, S, dim) = (B, T, dim) weighted sum\n",
    "        x = torch.bmm(attn_scores, encoder_outputs)\n",
    "\n",
    "        # (B, T, dim)\n",
    "        x = torch.cat((x, inputs), dim=-1)\n",
    "        x = torch.tanh(self.output_proj(x)) # concat + linear + tanh\n",
    "        \n",
    "        # restore shape (B, T, dim) -> (T, B, dim)\n",
    "        return x.transpose(1,0), attn_scores\n",
    "    \n",
    "# Seq2Seq模型\n",
    "class Seq2Seq(FairseqEncoderDecoderModel):\n",
    "    def __init__(self, args, encoder, decoder):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.args = args\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        src_tokens,\n",
    "        src_lengths,\n",
    "        prev_output_tokens,\n",
    "        return_all_hiddens: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run the forward pass for an encoder-decoder model.\n",
    "        \"\"\"\n",
    "        encoder_out = self.encoder(\n",
    "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
    "        )\n",
    "        logits, extra = self.decoder(\n",
    "            prev_output_tokens,\n",
    "            encoder_out=encoder_out,\n",
    "            src_lengths=src_lengths,\n",
    "            return_all_hiddens=return_all_hiddens,\n",
    "        )\n",
    "        return logits, extra\n",
    "    \n",
    "# 模型初始化\n",
    "from fairseq.models.transformer import TransformerEncoder, TransformerDecoder\n",
    "\n",
    "def build_model(args, task):\n",
    "    \"\"\" build a model instance based on hyperparameters \"\"\"\n",
    "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
    "\n",
    "    # token embeddings\n",
    "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
    "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
    "    \n",
    "    # encoder decoder\n",
    "    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
    "    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
    "\n",
    "    # sequence to sequence model\n",
    "    model = Seq2Seq(args, encoder, decoder)\n",
    "    \n",
    "    # initialization for seq2seq model is important, requires extra handling\n",
    "    def init_params(module):\n",
    "        from fairseq.modules import MultiheadAttention\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        if isinstance(module, MultiheadAttention):\n",
    "            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.RNNBase):\n",
    "            for name, param in module.named_parameters():\n",
    "                if \"weight\" in name or \"bias\" in name:\n",
    "                    param.data.uniform_(-0.1, 0.1)\n",
    "            \n",
    "    # weight initialization\n",
    "    model.apply(init_params)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 10:40:05 | INFO | hw5_transformer | Seq2Seq(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(8000, 256, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(8000, 256, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_projection): Linear(in_features=256, out_features=8000, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "arch_args = Namespace(\n",
    "    encoder_embed_dim=256,\n",
    "    encoder_ffn_embed_dim=512,\n",
    "    encoder_layers=6,\n",
    "    decoder_embed_dim=256,\n",
    "    decoder_ffn_embed_dim=1024,\n",
    "    decoder_layers=6,\n",
    "    share_decoder_input_output_embed=True,\n",
    "    dropout=0.3,\n",
    ")\n",
    "\n",
    "def add_transformer_args(args):\n",
    "    args.encoder_attention_heads=8\n",
    "    args.encoder_normalize_before=True\n",
    "    \n",
    "    args.decoder_attention_heads=8\n",
    "    args.decoder_normalize_before=True\n",
    "    \n",
    "    args.activation_fn=\"gelu\"\n",
    "    args.max_source_positions=1024\n",
    "    args.max_target_positions=1024\n",
    "    \n",
    "    # patches on default parameters for Transformer (those not set above)\n",
    "    from fairseq.models.transformer import base_architecture\n",
    "    base_architecture(arch_args)\n",
    "\n",
    "add_transformer_args(arch_args)\n",
    "     \n",
    "\n",
    "if config.use_wandb:\n",
    "    wandb.config.update(vars(arch_args))\n",
    "     \n",
    "\n",
    "model = build_model(arch_args, task)\n",
    "logger.info(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 接下来是各种优化\n",
    "\n",
    "### 损失函数：标签平滑化\n",
    "给ground truth外的标签也加上一点概率，可以减少过拟合的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://fairseq.readthedocs.io/en/latest/_modules/fairseq/criterions/label_smoothed_cross_entropy.html\n",
    "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
    "    def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduce = reduce\n",
    "    \n",
    "    def forward(self, lprobs, target):\n",
    "        if target.dim() == lprobs.dim() - 1:\n",
    "            target = target.unsqueeze(-1)\n",
    "        # nll: Negative log likelihood，the cross-entropy when target is one-hot. following line is same as F.nll_loss\n",
    "        nll_loss = -lprobs.gather(dim=-1, index=target)\n",
    "        #  reserve some probability for other labels. thus when calculating cross-entropy, \n",
    "        # equivalent to summing the log probs of all labels\n",
    "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
    "        if self.ignore_index is not None:\n",
    "            pad_mask = target.eq(self.ignore_index)\n",
    "            nll_loss.masked_fill_(pad_mask, 0.0)\n",
    "            smooth_loss.masked_fill_(pad_mask, 0.0)\n",
    "        else:\n",
    "            nll_loss = nll_loss.squeeze(-1)\n",
    "            smooth_loss = smooth_loss.squeeze(-1)\n",
    "        if self.reduce:\n",
    "            nll_loss = nll_loss.sum()\n",
    "            smooth_loss = smooth_loss.sum()\n",
    "        # when calculating cross-entropy, add the loss of other labels\n",
    "        eps_i = self.smoothing / lprobs.size(-1)\n",
    "        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
    "        return loss\n",
    "\n",
    "# generally, 0.1 is good enough\n",
    "criterion = LabelSmoothedCrossEntropyCriterion(\n",
    "    smoothing=0.1,\n",
    "    ignore_index=task.target_dictionary.pad(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器：Adam + learning rate scheduler\n",
    "采用warm-up的方法\n",
    "\n",
    "$$ \\text{lrate}=d_{\\text{model}}^{-0.5}\\times min(\\text{step-num}^{-0.5}, \\text{step-num}\\times \\text{warmup-steps}^{-1.5}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_rate(d_model, step_num, warmup_step):\n",
    "    # TODO: Change lr from constant to the equation shown above\n",
    "    # lr = 0.001\n",
    "    lr = pow(d_model, -0.5) * min(pow(step_num, -0.5), step_num * pow(warmup_step, -1.5))\n",
    "    return lr\n",
    "     \n",
    "\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "    \n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        return self.optimizer.param_groups\n",
    "        \n",
    "    def multiply_grads(self, c):\n",
    "        \"\"\"Multiplies grads by a constant *c*.\"\"\"                \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    p.grad.data.mul_(c)\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return 0 if not step else self.factor * get_rate(self.model_size, step, self.warmup)\n",
    "     \n",
    "# Scheduling Visualized\n",
    "optimizer = NoamOpt(\n",
    "    model_size=arch_args.encoder_embed_dim, \n",
    "    factor=config.lr_factor, \n",
    "    warmup=config.lr_warmup, \n",
    "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
    "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
    "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\n",
    "None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data import iterators\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n",
    "    itr = epoch_itr.next_epoch_itr(shuffle=True)\n",
    "    itr = iterators.GroupedIterator(itr, accum_steps) # gradient accumulation: update every accum_steps samples\n",
    "    \n",
    "    stats = {\"loss\": []}\n",
    "    scaler = GradScaler() # automatic mixed precision (amp) \n",
    "    \n",
    "    model.train()\n",
    "    progress = tqdm.tqdm(itr, desc=f\"train epoch {epoch_itr.epoch}\", leave=False)\n",
    "    for samples in progress:\n",
    "        model.zero_grad()\n",
    "        accum_loss = 0\n",
    "        sample_size = 0\n",
    "        # gradient accumulation: update every accum_steps samples\n",
    "        for i, sample in enumerate(samples):\n",
    "            if i == 1:\n",
    "                # emptying the CUDA cache after the first step can reduce the chance of OOM\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            sample = utils.move_to_cuda(sample, device=device)\n",
    "            target = sample[\"target\"]\n",
    "            sample_size_i = sample[\"ntokens\"]\n",
    "            sample_size += sample_size_i\n",
    "            \n",
    "            # mixed precision training\n",
    "            with autocast():\n",
    "                net_output = model.forward(**sample[\"net_input\"])\n",
    "                lprobs = F.log_softmax(net_output[0], -1)            \n",
    "                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n",
    "                \n",
    "                # logging\n",
    "                accum_loss += loss.item()\n",
    "                # back-prop\n",
    "                scaler.scale(loss).backward()                \n",
    "        \n",
    "        scaler.unscale_(optimizer)\n",
    "        optimizer.multiply_grads(1 / (sample_size or 1.0)) # (sample_size or 1.0) handles the case of a zero gradient\n",
    "        gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm) # grad norm clipping prevents gradient exploding\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # logging\n",
    "        loss_print = accum_loss/sample_size\n",
    "        stats[\"loss\"].append(loss_print)\n",
    "        progress.set_postfix(loss=loss_print)\n",
    "        if config.use_wandb:\n",
    "            wandb.log({\n",
    "                \"train/loss\": loss_print,\n",
    "                \"train/grad_norm\": gnorm.item(),\n",
    "                \"train/lr\": optimizer.rate(),\n",
    "                \"train/sample_size\": sample_size,\n",
    "            })\n",
    "        \n",
    "    loss_print = np.mean(stats[\"loss\"])\n",
    "    logger.info(f\"training loss: {loss_print:.4f}\")\n",
    "    return stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairseq's beam search generator\n",
    "# given model and input seqeunce, produce translation hypotheses by beam search\n",
    "sequence_generator = task.build_generator([model], config)\n",
    "\n",
    "def decode(toks, dictionary):\n",
    "    # convert from Tensor to human readable sentence\n",
    "    s = dictionary.string(\n",
    "        toks.int().cpu(),\n",
    "        config.post_process,\n",
    "    )\n",
    "    return s if s else \"\"\n",
    "\n",
    "def inference_step(sample, model):\n",
    "    gen_out = sequence_generator.generate([model], sample)\n",
    "    srcs = []\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    for i in range(len(gen_out)):\n",
    "        # for each sample, collect the input, hypothesis and reference, later be used to calculate BLEU\n",
    "        srcs.append(decode(\n",
    "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()), \n",
    "            task.source_dictionary,\n",
    "        ))\n",
    "        hyps.append(decode(\n",
    "            gen_out[i][0][\"tokens\"], # 0 indicates using the top hypothesis in beam\n",
    "            task.target_dictionary,\n",
    "        ))\n",
    "        refs.append(decode(\n",
    "            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()), \n",
    "            task.target_dictionary,\n",
    "        ))\n",
    "    return srcs, hyps, refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import sacrebleu\n",
    "\n",
    "def validate(model, task, criterion, log_to_wandb=True):\n",
    "    logger.info('begin validation')\n",
    "    itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
    "    \n",
    "    stats = {\"loss\":[], \"bleu\": 0, \"srcs\":[], \"hyps\":[], \"refs\":[]}\n",
    "    srcs = []\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    \n",
    "    model.eval()\n",
    "    progress = tqdm.tqdm(itr, desc=f\"validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(progress):\n",
    "            # validation loss\n",
    "            sample = utils.move_to_cuda(sample, device=device)\n",
    "            net_output = model.forward(**sample[\"net_input\"])\n",
    "\n",
    "            lprobs = F.log_softmax(net_output[0], -1)\n",
    "            target = sample[\"target\"]\n",
    "            sample_size = sample[\"ntokens\"]\n",
    "            loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n",
    "            progress.set_postfix(valid_loss=loss.item())\n",
    "            stats[\"loss\"].append(loss)\n",
    "            \n",
    "            # do inference\n",
    "            s, h, r = inference_step(sample, model)\n",
    "            srcs.extend(s)\n",
    "            hyps.extend(h)\n",
    "            refs.extend(r)\n",
    "            \n",
    "    tok = 'zh' if task.cfg.target_lang == 'zh' else '13a'\n",
    "    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n",
    "    stats[\"bleu\"] = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tok) # 計算BLEU score\n",
    "    stats[\"srcs\"] = srcs\n",
    "    stats[\"hyps\"] = hyps\n",
    "    stats[\"refs\"] = refs\n",
    "    \n",
    "    if config.use_wandb and log_to_wandb:\n",
    "        wandb.log({\n",
    "            \"valid/loss\": stats[\"loss\"],\n",
    "            \"valid/bleu\": stats[\"bleu\"].score,\n",
    "        }, commit=False)\n",
    "    \n",
    "    showid = np.random.randint(len(hyps))\n",
    "    logger.info(\"example source: \" + srcs[showid])\n",
    "    logger.info(\"example hypothesis: \" + hyps[showid])\n",
    "    logger.info(\"example reference: \" + refs[showid])\n",
    "    \n",
    "    # show bleu results\n",
    "    logger.info(f\"validation loss:\\t{stats['loss']:.4f}\")\n",
    "    logger.info(stats[\"bleu\"].format())\n",
    "    return stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_save(model, task, criterion, optimizer, epoch, save=True):   \n",
    "    stats = validate(model, task, criterion)\n",
    "    bleu = stats['bleu']\n",
    "    loss = stats['loss']\n",
    "    if save:\n",
    "        # save epoch checkpoints\n",
    "        savedir = Path(config.savedir).absolute()\n",
    "        savedir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        check = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"stats\": {\"bleu\": bleu.score, \"loss\": loss},\n",
    "            \"optim\": {\"step\": optimizer._step}\n",
    "        }\n",
    "        torch.save(check, savedir/f\"checkpoint{epoch}.pt\")\n",
    "        shutil.copy(savedir/f\"checkpoint{epoch}.pt\", savedir/f\"checkpoint_last.pt\")\n",
    "        logger.info(f\"saved epoch checkpoint: {savedir}/checkpoint{epoch}.pt\")\n",
    "    \n",
    "        # save epoch samples\n",
    "        with open(savedir/f\"samples{epoch}.{config.source_lang}-{config.target_lang}.txt\", \"w\") as f:\n",
    "            for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n",
    "                f.write(f\"{s}\\t{h}\\n\")\n",
    "\n",
    "        # get best valid bleu    \n",
    "        if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n",
    "            validate_and_save.best_bleu = bleu.score\n",
    "            torch.save(check, savedir/f\"checkpoint_best.pt\")\n",
    "            \n",
    "        del_file = savedir / f\"checkpoint{epoch - config.keep_last_epochs}.pt\"\n",
    "        if del_file.exists():\n",
    "            del_file.unlink()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def try_load_checkpoint(model, optimizer=None, name=None):\n",
    "    name = name if name else \"checkpoint_last.pt\"\n",
    "    checkpath = Path(config.savedir)/name\n",
    "    if checkpath.exists():\n",
    "        check = torch.load(checkpath)\n",
    "        model.load_state_dict(check[\"model\"])\n",
    "        stats = check[\"stats\"]\n",
    "        step = \"unknown\"\n",
    "        if optimizer != None:\n",
    "            optimizer._step = step = check[\"optim\"][\"step\"]\n",
    "        logger.info(f\"loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n",
    "    else:\n",
    "        logger.info(f\"no checkpoints found at {checkpath}!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 10:40:12 | INFO | hw5_transformer | task: TranslationTask\n",
      "2023-01-20 10:40:12 | INFO | hw5_transformer | encoder: TransformerEncoder\n",
      "2023-01-20 10:40:12 | INFO | hw5_transformer | decoder: TransformerDecoder\n",
      "2023-01-20 10:40:12 | INFO | hw5_transformer | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2023-01-20 10:40:12 | INFO | hw5_transformer | optimizer: NoamOpt\n",
      "2023-01-20 10:40:12 | INFO | hw5_transformer | num. model params: 13,580,288 (num. trained: 13,580,288)\n",
      "2023-01-20 10:40:12 | INFO | hw5_transformer | max tokens per batch = 8192, accumulate steps = 2\n",
      "2023-01-20 10:40:12 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[326674]\n",
      "2023-01-20 10:40:12 | INFO | hw5_transformer | no checkpoints found at checkpoints/transformer/checkpoint_last.pt!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0a967948a644d49b16eb60dd13d316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 1:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 10:49:38 | INFO | hw5_transformer | training loss: 7.0275\n",
      "2023-01-20 10:49:38 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f033811630d4b05bcc8eb074776f9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruizhe/miniconda3/envs/d2l-zh/lib/python3.8/site-packages/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  beams_buf = indices_buf // vocab_size\n",
      "/home/ruizhe/miniconda3/envs/d2l-zh/lib/python3.8/site-packages/fairseq/sequence_generator.py:657: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  unfin_idx = idx // beam_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 10:50:14 | INFO | hw5_transformer | example source: but the point is , it doesn't feel that way .\n",
      "2023-01-20 10:50:14 | INFO | hw5_transformer | example hypothesis: 但 , 但 , 但 , 但 , 很棒 。\n",
      "2023-01-20 10:50:14 | INFO | hw5_transformer | example reference: 但重點是 , 感覺起來卻不是那麼回事呀\n",
      "2023-01-20 10:50:14 | INFO | hw5_transformer | validation loss:\t5.9515\n",
      "2023-01-20 10:50:14 | INFO | hw5_transformer | BLEU = 0.99 20.2/3.3/0.7/0.1 (BP = 0.651 ratio = 0.700 hyp_len = 78230 ref_len = 111811)\n",
      "2023-01-20 10:50:14 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint1.pt\n",
      "2023-01-20 10:50:15 | INFO | hw5_transformer | end of epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50988726620a4a83860aa1382018e953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 2:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 10:59:38 | INFO | hw5_transformer | training loss: 5.4489\n",
      "2023-01-20 10:59:38 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fb44238dc5411bace5a288bfdffbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 11:00:09 | INFO | hw5_transformer | example source: we can push through that .\n",
      "2023-01-20 11:00:09 | INFO | hw5_transformer | example hypothesis: 我們可以把這個轉換 。\n",
      "2023-01-20 11:00:09 | INFO | hw5_transformer | example reference: 我們可以衝破這些限制 。\n",
      "2023-01-20 11:00:09 | INFO | hw5_transformer | validation loss:\t4.8927\n",
      "2023-01-20 11:00:09 | INFO | hw5_transformer | BLEU = 7.98 41.1/15.9/6.6/2.9 (BP = 0.757 ratio = 0.782 hyp_len = 87430 ref_len = 111811)\n",
      "2023-01-20 11:00:09 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint2.pt\n",
      "2023-01-20 11:00:09 | INFO | hw5_transformer | end of epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d074c092187943b6b81d14d84f77fc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 3:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 11:09:30 | INFO | hw5_transformer | training loss: 4.8273\n",
      "2023-01-20 11:09:30 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217208aa4648498b8e265e0cffb20772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 11:10:04 | INFO | hw5_transformer | example source: thank you .\n",
      "2023-01-20 11:10:04 | INFO | hw5_transformer | example hypothesis: 謝謝 。\n",
      "2023-01-20 11:10:04 | INFO | hw5_transformer | example reference: 謝謝 。\n",
      "2023-01-20 11:10:04 | INFO | hw5_transformer | validation loss:\t4.4599\n",
      "2023-01-20 11:10:04 | INFO | hw5_transformer | BLEU = 11.78 44.4/19.2/9.1/4.6 (BP = 0.860 ratio = 0.869 hyp_len = 97136 ref_len = 111811)\n",
      "2023-01-20 11:10:04 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint3.pt\n",
      "2023-01-20 11:10:04 | INFO | hw5_transformer | end of epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3146c61c292c47409086f6f373074dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 4:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 11:19:32 | INFO | hw5_transformer | training loss: 4.5068\n",
      "2023-01-20 11:19:32 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f56547360c417ba9da8aea381311f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 11:20:11 | INFO | hw5_transformer | example source: those men would eventually be convicted of placing a van filled with 1 , 500 pounds of explosives into the sublevel parking lot of the world trade center's north tower , causing an explosion that killed six people and injured over 1 , 000 others .\n",
      "2023-01-20 11:20:11 | INFO | hw5_transformer | example hypothesis: 這些男性最終會被判斷 , 用1500磅的優勢 , 到世界上許多貿易中心 , 造成許多貿易貿易中心 , 包括北部 , 到北部 , 殺死了六萬人 , 殺死了超過六萬人 。\n",
      "2023-01-20 11:20:11 | INFO | hw5_transformer | example reference: 這群男人最終被指控將滿載1500磅重的炸彈的廂式貨車停在世界貿易中心北塔的地下停車場 , 爆炸造成6人死亡 , 同時致使超過1000人受傷 。\n",
      "2023-01-20 11:20:11 | INFO | hw5_transformer | validation loss:\t4.1878\n",
      "2023-01-20 11:20:11 | INFO | hw5_transformer | BLEU = 14.95 46.0/21.5/10.8/5.7 (BP = 0.949 ratio = 0.951 hyp_len = 106297 ref_len = 111811)\n",
      "2023-01-20 11:20:11 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint4.pt\n",
      "2023-01-20 11:20:11 | INFO | hw5_transformer | end of epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffadb90ccb8d4b1c945f0d03329523df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 5:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 11:29:36 | INFO | hw5_transformer | training loss: 4.2924\n",
      "2023-01-20 11:29:36 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a77a5ebdf74a958fd4c5fe3bcb4dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 11:30:16 | INFO | hw5_transformer | example source: but this is really how i go about creating these photographs .\n",
      "2023-01-20 11:30:16 | INFO | hw5_transformer | example hypothesis: 但這真的是我如何創造這些照片的照片 。\n",
      "2023-01-20 11:30:16 | INFO | hw5_transformer | example reference: 但是我真的是這樣做出這些相片的 。\n",
      "2023-01-20 11:30:16 | INFO | hw5_transformer | validation loss:\t4.0538\n",
      "2023-01-20 11:30:16 | INFO | hw5_transformer | BLEU = 17.00 47.9/23.4/12.3/6.7 (BP = 0.974 ratio = 0.974 hyp_len = 108932 ref_len = 111811)\n",
      "2023-01-20 11:30:16 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint5.pt\n",
      "2023-01-20 11:30:16 | INFO | hw5_transformer | end of epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97521ce57814e6a8bad6b53d57eefde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 6:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 11:39:40 | INFO | hw5_transformer | training loss: 4.1441\n",
      "2023-01-20 11:39:40 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51ad14d84af466bab1ec54c66a5c0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 11:40:16 | INFO | hw5_transformer | example source: so that is a fact .\n",
      "2023-01-20 11:40:16 | INFO | hw5_transformer | example hypothesis: 所以這是事實 。\n",
      "2023-01-20 11:40:16 | INFO | hw5_transformer | example reference: 這就是事實\n",
      "2023-01-20 11:40:16 | INFO | hw5_transformer | validation loss:\t3.9090\n",
      "2023-01-20 11:40:16 | INFO | hw5_transformer | BLEU = 18.44 51.5/26.0/13.9/7.9 (BP = 0.940 ratio = 0.942 hyp_len = 105335 ref_len = 111811)\n",
      "2023-01-20 11:40:17 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint6.pt\n",
      "2023-01-20 11:40:17 | INFO | hw5_transformer | end of epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d479aa8a8349f68a7737aea3ba5e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 7:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 11:49:40 | INFO | hw5_transformer | training loss: 4.0051\n",
      "2023-01-20 11:49:40 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b36001ffa04fcc94f93a72d23dacf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 11:50:16 | INFO | hw5_transformer | example source: thank you so much everyone .\n",
      "2023-01-20 11:50:16 | INFO | hw5_transformer | example hypothesis: 感謝大家 。\n",
      "2023-01-20 11:50:16 | INFO | hw5_transformer | example reference: 謝謝大家 !\n",
      "2023-01-20 11:50:16 | INFO | hw5_transformer | validation loss:\t3.8232\n",
      "2023-01-20 11:50:16 | INFO | hw5_transformer | BLEU = 18.96 52.7/27.1/14.8/8.5 (BP = 0.920 ratio = 0.923 hyp_len = 103249 ref_len = 111811)\n",
      "2023-01-20 11:50:16 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint7.pt\n",
      "2023-01-20 11:50:17 | INFO | hw5_transformer | end of epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6da3581bc9e45c8bbb771430b2d3418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 8:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 11:59:43 | INFO | hw5_transformer | training loss: 3.9069\n",
      "2023-01-20 11:59:43 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be45f0b514ca4a538d1144d54ecb829d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 12:00:18 | INFO | hw5_transformer | example source: these differences became a source of inspired artistic celebration .\n",
      "2023-01-20 12:00:18 | INFO | hw5_transformer | example hypothesis: 這些差異變成了靈感藝術慶祝的來源 。\n",
      "2023-01-20 12:00:18 | INFO | hw5_transformer | example reference: 這些差異變成了一個源頭 , 富有靈感之藝術讚頌的源頭 。\n",
      "2023-01-20 12:00:18 | INFO | hw5_transformer | validation loss:\t3.7368\n",
      "2023-01-20 12:00:18 | INFO | hw5_transformer | BLEU = 19.53 55.1/29.0/16.0/9.3 (BP = 0.885 ratio = 0.891 hyp_len = 99659 ref_len = 111811)\n",
      "2023-01-20 12:00:18 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint8.pt\n",
      "2023-01-20 12:00:18 | INFO | hw5_transformer | end of epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54916d639fd4d2bb20cc8b27a8566eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 9:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 12:09:43 | INFO | hw5_transformer | training loss: 3.8316\n",
      "2023-01-20 12:09:43 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae855cf6f6674e93916526c12cd0efee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 12:10:21 | INFO | hw5_transformer | example source: it's not .\n",
      "2023-01-20 12:10:21 | INFO | hw5_transformer | example hypothesis: 不是 。\n",
      "2023-01-20 12:10:21 | INFO | hw5_transformer | example reference: 不是\n",
      "2023-01-20 12:10:21 | INFO | hw5_transformer | validation loss:\t3.7037\n",
      "2023-01-20 12:10:21 | INFO | hw5_transformer | BLEU = 21.05 53.6/28.2/15.7/9.2 (BP = 0.974 ratio = 0.975 hyp_len = 108971 ref_len = 111811)\n",
      "2023-01-20 12:10:21 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint9.pt\n",
      "2023-01-20 12:10:21 | INFO | hw5_transformer | end of epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ad24bc2c1843bb9ffd4fe763f9c438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 10:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 12:19:44 | INFO | hw5_transformer | training loss: 3.7740\n",
      "2023-01-20 12:19:44 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f8a1abb3004ae1a22f76efdbae563e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 12:20:19 | INFO | hw5_transformer | example source: so think about when you have friends , families and coworkers in california , on the west coast or in other parts of the world .\n",
      "2023-01-20 12:20:19 | INFO | hw5_transformer | example hypothesis: 所以 , 想想你有朋友、家人、在加州的同事、在西岸或其他地區的西岸 。\n",
      "2023-01-20 12:20:19 | INFO | hw5_transformer | example reference: 所以 , 想像當你嘗試聯繫在加州 , 在西海岸或者在世界的另一面\n",
      "2023-01-20 12:20:19 | INFO | hw5_transformer | validation loss:\t3.6461\n",
      "2023-01-20 12:20:19 | INFO | hw5_transformer | BLEU = 21.40 55.7/29.8/16.8/9.9 (BP = 0.933 ratio = 0.936 hyp_len = 104612 ref_len = 111811)\n",
      "2023-01-20 12:20:19 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint10.pt\n",
      "2023-01-20 12:20:19 | INFO | hw5_transformer | end of epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9906d8a3cb1048db92eacc1d8842f63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 11:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 12:29:44 | INFO | hw5_transformer | training loss: 3.7276\n",
      "2023-01-20 12:29:44 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3abb34c738434ba4feb76c12ca588b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 12:30:25 | INFO | hw5_transformer | example source: and i was asked to move out because one of my roommates had shared my status with her parents .\n",
      "2023-01-20 12:30:25 | INFO | hw5_transformer | example hypothesis: 我被要求離開 , 因為我的其中一位室友和她的父母分享我的地位 。\n",
      "2023-01-20 12:30:25 | INFO | hw5_transformer | example reference: 我會被要求搬出去 , 是因為其中一名室友和她的父母談了我的狀況 。\n",
      "2023-01-20 12:30:25 | INFO | hw5_transformer | validation loss:\t3.6226\n",
      "2023-01-20 12:30:25 | INFO | hw5_transformer | BLEU = 22.02 55.0/29.5/16.7/9.9 (BP = 0.966 ratio = 0.967 hyp_len = 108114 ref_len = 111811)\n",
      "2023-01-20 12:30:25 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint11.pt\n",
      "2023-01-20 12:30:25 | INFO | hw5_transformer | end of epoch 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa1859c7d78421a84922e5d2802c325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 12:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 12:39:52 | INFO | hw5_transformer | training loss: 3.6910\n",
      "2023-01-20 12:39:52 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da1ee3bc6dd47f495a87965787395e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 12:40:28 | INFO | hw5_transformer | example source: and i want to share with you a quick video from our facility that gives you a sense of how this looks at scale .\n",
      "2023-01-20 12:40:28 | INFO | hw5_transformer | example hypothesis: 我想要和各位分享一個快速的影片 , 可以讓你了解這個規模的規模 。\n",
      "2023-01-20 12:40:28 | INFO | hw5_transformer | example reference: 我想與你們快速分享一段關於我們設備的影片可以瞭解一下它的規模\n",
      "2023-01-20 12:40:28 | INFO | hw5_transformer | validation loss:\t3.5792\n",
      "2023-01-20 12:40:28 | INFO | hw5_transformer | BLEU = 22.16 56.1/30.3/17.3/10.4 (BP = 0.943 ratio = 0.945 hyp_len = 105630 ref_len = 111811)\n",
      "2023-01-20 12:40:28 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint12.pt\n",
      "2023-01-20 12:40:28 | INFO | hw5_transformer | end of epoch 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ce333972d7423c8b29f2b6369d6aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 13:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 12:49:45 | INFO | hw5_transformer | training loss: 3.6577\n",
      "2023-01-20 12:49:45 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf39fc99e3346f8904c7b41eddd69d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 12:50:20 | INFO | hw5_transformer | example source: it's a distinction perhaps we might make between two nobel laureates , richard feynman and john nash .\n",
      "2023-01-20 12:50:20 | INFO | hw5_transformer | example hypothesis: 可能是兩位諾貝爾獎得主 , 理查費曼和約翰納什 。\n",
      "2023-01-20 12:50:20 | INFO | hw5_transformer | example reference: 我們或許可以從兩位諾貝爾獎得主 , 理查費曼和約翰奈許之間看出這樣的差異 。\n",
      "2023-01-20 12:50:20 | INFO | hw5_transformer | validation loss:\t3.5645\n",
      "2023-01-20 12:50:20 | INFO | hw5_transformer | BLEU = 22.11 57.2/31.1/17.8/10.8 (BP = 0.914 ratio = 0.918 hyp_len = 102624 ref_len = 111811)\n",
      "2023-01-20 12:50:20 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint13.pt\n",
      "2023-01-20 12:50:20 | INFO | hw5_transformer | end of epoch 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a984f2efb3405890bc1b3d34487e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 14:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 12:59:45 | INFO | hw5_transformer | training loss: 3.6304\n",
      "2023-01-20 12:59:45 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f563bdc61af64a7d9b896943dc1b0d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 13:00:22 | INFO | hw5_transformer | example source: and therein lie the words of wisdom of a young girl whose brief life forever impacted mine .\n",
      "2023-01-20 13:00:22 | INFO | hw5_transformer | example hypothesis: 還有一個年輕女孩的智慧 , 永遠影響我 。\n",
      "2023-01-20 13:00:22 | INFO | hw5_transformer | example reference: 她說的話對我有很大啟發 。 這一個年幼女孩短暫的一生永遠影響著我 。\n",
      "2023-01-20 13:00:22 | INFO | hw5_transformer | validation loss:\t3.5491\n",
      "2023-01-20 13:00:22 | INFO | hw5_transformer | BLEU = 21.63 58.4/31.9/18.2/11.0 (BP = 0.876 ratio = 0.883 hyp_len = 98725 ref_len = 111811)\n",
      "2023-01-20 13:00:22 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint14.pt\n",
      "2023-01-20 13:00:23 | INFO | hw5_transformer | end of epoch 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da2b779bd594cf9a110b88590ec99c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 15:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 13:09:49 | INFO | hw5_transformer | training loss: 3.6115\n",
      "2023-01-20 13:09:49 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb2f081bd8a48cf88bca2237d8dbf3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 13:10:23 | INFO | hw5_transformer | example source: and we've started making curtains , and not only is it beautiful , but people can see status that you care about your children .\n",
      "2023-01-20 13:10:23 | INFO | hw5_transformer | example hypothesis: 我們開始做曲線 , 不僅是美麗的 , 人們還能看到你在乎你的孩子的現狀 。\n",
      "2023-01-20 13:10:23 | INFO | hw5_transformer | example reference: 我們也開始生産窗簾不止因爲窗簾看起來漂亮 , 人們也可以看到你的地位看得出你很關心小孩\n",
      "2023-01-20 13:10:23 | INFO | hw5_transformer | validation loss:\t3.5343\n",
      "2023-01-20 13:10:23 | INFO | hw5_transformer | BLEU = 22.31 57.9/31.8/18.3/11.0 (BP = 0.904 ratio = 0.908 hyp_len = 101551 ref_len = 111811)\n",
      "2023-01-20 13:10:23 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint15.pt\n",
      "2023-01-20 13:10:23 | INFO | hw5_transformer | end of epoch 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521b970e4c5b4f8a95f25b418087ced8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 16:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 13:19:44 | INFO | hw5_transformer | training loss: 3.5849\n",
      "2023-01-20 13:19:44 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9c6ef83c754c39a5eaad16a1c2c7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 13:20:23 | INFO | hw5_transformer | example source: now , i'm not suggesting we want to raise our babies in our stomach , but i am suggesting it's possible we might want to manage gastric secretion in the gut .\n",
      "2023-01-20 13:20:23 | INFO | hw5_transformer | example hypothesis: 我並不是建議我們要在我們的胃裡養育我們的寶寶 , 但我建議 , 我們可能想在腸道上管理天然氣污染 。\n",
      "2023-01-20 13:20:23 | INFO | hw5_transformer | example reference: 我並不是說要把嬰兒養在胃裡而是我們可能可以了解胃在內臟裡的分泌物\n",
      "2023-01-20 13:20:23 | INFO | hw5_transformer | validation loss:\t3.5184\n",
      "2023-01-20 13:20:23 | INFO | hw5_transformer | BLEU = 23.15 56.9/31.2/18.0/11.0 (BP = 0.950 ratio = 0.952 hyp_len = 106404 ref_len = 111811)\n",
      "2023-01-20 13:20:23 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint16.pt\n",
      "2023-01-20 13:20:23 | INFO | hw5_transformer | end of epoch 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6004d3c36042b1acc8959a11d74425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 17:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 13:29:44 | INFO | hw5_transformer | training loss: 3.5665\n",
      "2023-01-20 13:29:44 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22479db9a7564b33a4cb6c37554da5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 13:30:24 | INFO | hw5_transformer | example source: so we're hoping that's what they'll do .\n",
      "2023-01-20 13:30:24 | INFO | hw5_transformer | example hypothesis: 所以 , 我們希望這是他們會做的 。\n",
      "2023-01-20 13:30:24 | INFO | hw5_transformer | example reference: 所以 , 我們希望它們能夠幫忙 。\n",
      "2023-01-20 13:30:24 | INFO | hw5_transformer | validation loss:\t3.5092\n",
      "2023-01-20 13:30:24 | INFO | hw5_transformer | BLEU = 23.36 56.5/30.9/17.8/10.9 (BP = 0.968 ratio = 0.969 hyp_len = 108334 ref_len = 111811)\n",
      "2023-01-20 13:30:25 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint17.pt\n",
      "2023-01-20 13:30:25 | INFO | hw5_transformer | end of epoch 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4895587d79a14b6f9bfe4a0bb9a16278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 18:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 13:39:44 | INFO | hw5_transformer | training loss: 3.5487\n",
      "2023-01-20 13:39:44 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83173b4f7e8f4e6e8b756cbe17c0b11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 13:40:17 | INFO | hw5_transformer | example source: i remember so vividly that day .\n",
      "2023-01-20 13:40:17 | INFO | hw5_transformer | example hypothesis: 那天我非常清楚記得 。\n",
      "2023-01-20 13:40:17 | INFO | hw5_transformer | example reference: 我對那一天還記憶猶新 。\n",
      "2023-01-20 13:40:17 | INFO | hw5_transformer | validation loss:\t3.5054\n",
      "2023-01-20 13:40:17 | INFO | hw5_transformer | BLEU = 22.88 58.2/32.1/18.6/11.4 (BP = 0.912 ratio = 0.916 hyp_len = 102415 ref_len = 111811)\n",
      "2023-01-20 13:40:17 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint18.pt\n",
      "2023-01-20 13:40:17 | INFO | hw5_transformer | end of epoch 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17361bb1282a48e89ba48f5e18049445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 19:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 13:49:45 | INFO | hw5_transformer | training loss: 3.5333\n",
      "2023-01-20 13:49:45 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b490583681d43f287f1f9f0e15980d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 13:50:19 | INFO | hw5_transformer | example source: the next step is to make compliant toes , and try to add spines and claws and set it for dry adhesives .\n",
      "2023-01-20 13:50:19 | INFO | hw5_transformer | example hypothesis: 下一步是要做出複雜的腳趾 , 試著加上脊椎和爪子 , 為乾燥的黏著劑設定 。\n",
      "2023-01-20 13:50:19 | INFO | hw5_transformer | example reference: 下一步是要製造順從聽話的腳趾頭 。 然後加上刺和爪子 , 再加上乾膠 。\n",
      "2023-01-20 13:50:19 | INFO | hw5_transformer | validation loss:\t3.4779\n",
      "2023-01-20 13:50:19 | INFO | hw5_transformer | BLEU = 23.38 57.8/32.0/18.6/11.4 (BP = 0.934 ratio = 0.936 hyp_len = 104674 ref_len = 111811)\n",
      "2023-01-20 13:50:19 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint19.pt\n",
      "2023-01-20 13:50:19 | INFO | hw5_transformer | end of epoch 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854b219baab942bbafb689f0f79959f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 20:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 13:59:44 | INFO | hw5_transformer | training loss: 3.5178\n",
      "2023-01-20 13:59:44 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1994f0f901540cca5bd593b969c9a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 14:00:19 | INFO | hw5_transformer | example source: and got the cord off of the baby's neck , and a healthy screaming , kicking baby arrived , just as the dad ran in from the parking lot , \" hi , you have a son , i'm dr . darria .\n",
      "2023-01-20 14:00:19 | INFO | hw5_transformer | example hypothesis: 把嬰兒的脖子從嬰兒的脖子裡拿走 , 健康的尖叫 , 刀刀寶從停車場跑進來 , 「 嗨 , 你有兒子 , 我是darria醫生 。\n",
      "2023-01-20 14:00:19 | INFO | hw5_transformer | example reference: 我把臍帶從嬰兒的脖子繞開 , 接著 , 一個健康哭鬧的嬰兒誕生了 。 同時 , 他的父親剛從停車場趕來 。 「 你好 , 是一個男孩 。 我是達里亞醫生 。\n",
      "2023-01-20 14:00:19 | INFO | hw5_transformer | validation loss:\t3.4775\n",
      "2023-01-20 14:00:19 | INFO | hw5_transformer | BLEU = 23.55 57.3/31.7/18.5/11.3 (BP = 0.949 ratio = 0.950 hyp_len = 106197 ref_len = 111811)\n",
      "2023-01-20 14:00:19 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint20.pt\n",
      "2023-01-20 14:00:19 | INFO | hw5_transformer | end of epoch 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7eb09e32b334f69b6fb39be66bfcbbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 21:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 14:09:47 | INFO | hw5_transformer | training loss: 3.5081\n",
      "2023-01-20 14:09:47 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10319be487444afeb2f744415ef759cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 14:10:22 | INFO | hw5_transformer | example source: no , but the issue of city building in democracy is interesting because it creates chaos , right ?\n",
      "2023-01-20 14:10:22 | INFO | hw5_transformer | example hypothesis: 不 , 但在民主中 , 城市建築的問題很有趣 , 因為它創造混亂 , 對吧 ?\n",
      "2023-01-20 14:10:22 | INFO | hw5_transformer | example reference: 不 , 但民主的城市建設很有趣 , 因為會造成混亂 , 是嗎 ?\n",
      "2023-01-20 14:10:22 | INFO | hw5_transformer | validation loss:\t3.4671\n",
      "2023-01-20 14:10:22 | INFO | hw5_transformer | BLEU = 23.90 57.2/31.7/18.4/11.3 (BP = 0.965 ratio = 0.966 hyp_len = 107982 ref_len = 111811)\n",
      "2023-01-20 14:10:22 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint21.pt\n",
      "2023-01-20 14:10:22 | INFO | hw5_transformer | end of epoch 21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd696fab2a44ee99fb4bc6ade2d282a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 22:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 14:19:42 | INFO | hw5_transformer | training loss: 3.4904\n",
      "2023-01-20 14:19:42 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110da391463744c5ab6aa688607f326b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 14:20:16 | INFO | hw5_transformer | example source: so i'm going to show you a demo of this .\n",
      "2023-01-20 14:20:16 | INFO | hw5_transformer | example hypothesis: 所以我要給你們看一個示範 。\n",
      "2023-01-20 14:20:16 | INFO | hw5_transformer | example reference: 現在請各位看示範\n",
      "2023-01-20 14:20:16 | INFO | hw5_transformer | validation loss:\t3.4682\n",
      "2023-01-20 14:20:16 | INFO | hw5_transformer | BLEU = 23.70 58.0/32.2/18.8/11.5 (BP = 0.941 ratio = 0.942 hyp_len = 105362 ref_len = 111811)\n",
      "2023-01-20 14:20:16 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint22.pt\n",
      "2023-01-20 14:20:16 | INFO | hw5_transformer | end of epoch 22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c681a65792b94a4bb550cdc9e2b0b3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 23:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 14:29:37 | INFO | hw5_transformer | training loss: 3.4814\n",
      "2023-01-20 14:29:37 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd655e2c0b614b439e815bc57247b9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 14:30:14 | INFO | hw5_transformer | example source: you see , in every country where you hear about armed jihadis targeting civilians , there are also unarmed people defying those militants that you don't hear about , and those people need our support to succeed .\n",
      "2023-01-20 14:30:14 | INFO | hw5_transformer | example hypothesis: 你看 , 在每個國家 , 你聽到武裝的聖戰人員 , 也有一些未武裝的人 , 那些你聽不到的軍人 , 而那些人需要我們的支持才能成功 。\n",
      "2023-01-20 14:30:14 | INFO | hw5_transformer | example reference: 你看 , 不管在哪個國家你都會聽到武裝聖戰者針對平民百姓 , 也有很多手無寸鐵的人民公然反抗那些激進分子 , 只是你沒聽過 , 那些人需要我們的支持才能成功 。\n",
      "2023-01-20 14:30:14 | INFO | hw5_transformer | validation loss:\t3.4581\n",
      "2023-01-20 14:30:14 | INFO | hw5_transformer | BLEU = 23.34 58.8/32.7/19.1/11.7 (BP = 0.911 ratio = 0.915 hyp_len = 102305 ref_len = 111811)\n",
      "2023-01-20 14:30:14 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint23.pt\n",
      "2023-01-20 14:30:14 | INFO | hw5_transformer | end of epoch 23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb73628eba22439daaebe6adecbf3ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 24:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 14:39:37 | INFO | hw5_transformer | training loss: 3.4693\n",
      "2023-01-20 14:39:37 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3792b10bb9c4444876b7b45dfe6871c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 14:40:11 | INFO | hw5_transformer | example source: from gezi to tahrir to elsewhere , i've seen people put their lives and livelihoods on the line .\n",
      "2023-01-20 14:40:11 | INFO | hw5_transformer | example hypothesis: 從gezi到tahrir到其他地方 , 我見過人們把他們的生活和生活放在線上 。\n",
      "2023-01-20 14:40:11 | INFO | hw5_transformer | example reference: 從格濟公園 , 到解放廣場 , 再到其它地方 , 我見過許多人用他們的生命和生計做賭注 。\n",
      "2023-01-20 14:40:11 | INFO | hw5_transformer | validation loss:\t3.4582\n",
      "2023-01-20 14:40:11 | INFO | hw5_transformer | BLEU = 23.68 58.4/32.5/19.0/11.7 (BP = 0.931 ratio = 0.933 hyp_len = 104325 ref_len = 111811)\n",
      "2023-01-20 14:40:11 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint24.pt\n",
      "2023-01-20 14:40:11 | INFO | hw5_transformer | end of epoch 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e94959cfbe4992a15cf6b2c24a4303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 25:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 14:49:36 | INFO | hw5_transformer | training loss: 3.4615\n",
      "2023-01-20 14:49:36 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50c775a405d4016b72b3d0821e3a7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 14:50:16 | INFO | hw5_transformer | example source: and every project before that had been completely personal and it was a revelation when people just started commenting , started giving feedback on your code .\n",
      "2023-01-20 14:50:16 | INFO | hw5_transformer | example hypothesis: 每個專案都已經完全個人化了 , 大家開始評論 , 開始回饋你的程式碼 。\n",
      "2023-01-20 14:50:16 | INFO | hw5_transformer | example reference: 在那之前每一件計劃都是我個人的東西 , 所以當大家開始評論 , 開始對你的程式給意見時 , 真的是一種啟示 。\n",
      "2023-01-20 14:50:16 | INFO | hw5_transformer | validation loss:\t3.4368\n",
      "2023-01-20 14:50:16 | INFO | hw5_transformer | BLEU = 24.26 57.2/31.8/18.6/11.5 (BP = 0.972 ratio = 0.972 hyp_len = 108694 ref_len = 111811)\n",
      "2023-01-20 14:50:16 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint25.pt\n",
      "2023-01-20 14:50:16 | INFO | hw5_transformer | end of epoch 25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1bfde15834457f807ca4e44b8598dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 26:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 14:59:39 | INFO | hw5_transformer | training loss: 3.4528\n",
      "2023-01-20 14:59:39 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00e3826ecf343ad9231281b0009f507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:00:13 | INFO | hw5_transformer | example source: thank you so much .\n",
      "2023-01-20 15:00:13 | INFO | hw5_transformer | example hypothesis: 謝謝\n",
      "2023-01-20 15:00:13 | INFO | hw5_transformer | example reference: 感謝各位.\n",
      "2023-01-20 15:00:13 | INFO | hw5_transformer | validation loss:\t3.4432\n",
      "2023-01-20 15:00:13 | INFO | hw5_transformer | BLEU = 23.34 59.8/33.5/19.7/12.2 (BP = 0.886 ratio = 0.892 hyp_len = 99700 ref_len = 111811)\n",
      "2023-01-20 15:00:13 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint26.pt\n",
      "2023-01-20 15:00:13 | INFO | hw5_transformer | end of epoch 26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794a1b8a41984d87bf4f55b3c554d04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 27:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:09:36 | INFO | hw5_transformer | training loss: 3.4456\n",
      "2023-01-20 15:09:36 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab15027c60ff4c0a8e1faf31f2e0fe1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:10:16 | INFO | hw5_transformer | example source: at the age of six months , virtually every one of us is able to differentiate between animate and inanimate objects .\n",
      "2023-01-20 15:10:16 | INFO | hw5_transformer | example hypothesis: 在六個月之內 , 我們每一個人都能分辨出動畫和無生命物體之間的差異 。\n",
      "2023-01-20 15:10:16 | INFO | hw5_transformer | example reference: 在六個月大時 , 幾乎每個人都能辨別東西是否有生命 。\n",
      "2023-01-20 15:10:16 | INFO | hw5_transformer | validation loss:\t3.4341\n",
      "2023-01-20 15:10:16 | INFO | hw5_transformer | BLEU = 24.02 58.8/33.0/19.4/12.0 (BP = 0.927 ratio = 0.929 hyp_len = 103919 ref_len = 111811)\n",
      "2023-01-20 15:10:16 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint27.pt\n",
      "2023-01-20 15:10:16 | INFO | hw5_transformer | end of epoch 27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3965311970a34ba6bf2780f21d3babe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 28:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:19:42 | INFO | hw5_transformer | training loss: 3.4345\n",
      "2023-01-20 15:19:42 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bfee90d2a14293923a4ef65db50741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:20:18 | INFO | hw5_transformer | example source: the problem is there is no such thing as a viable democracy made up of experts , zealots , politicians and spectators .\n",
      "2023-01-20 15:20:18 | INFO | hw5_transformer | example hypothesis: 問題是 , 沒有一個可行的民主是由專家、零位、政治人物及觀點組成的 。\n",
      "2023-01-20 15:20:18 | INFO | hw5_transformer | example reference: 事實上 , 根本沒有一種民主制度是由專家 , 狂熱分子 , 政治家和旁觀者組成的\n",
      "2023-01-20 15:20:18 | INFO | hw5_transformer | validation loss:\t3.4246\n",
      "2023-01-20 15:20:18 | INFO | hw5_transformer | BLEU = 24.26 57.9/32.3/18.9/11.7 (BP = 0.956 ratio = 0.957 hyp_len = 106998 ref_len = 111811)\n",
      "2023-01-20 15:20:19 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint28.pt\n",
      "2023-01-20 15:20:19 | INFO | hw5_transformer | end of epoch 28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df053deca7c6450ba5781df12f42fd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 29:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:29:37 | INFO | hw5_transformer | training loss: 3.4273\n",
      "2023-01-20 15:29:37 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f95c6361d2b45748c473bcae076621a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:30:11 | INFO | hw5_transformer | example source: they're learning our culture , our language , our heritage and realizing we're just as different and just the same as each other .\n",
      "2023-01-20 15:30:11 | INFO | hw5_transformer | example hypothesis: 他們在學習我們的文化、我們的語言、我們的語言、我們的遺產 , 並了解到我們和彼此的差異 , 和彼此一樣 。\n",
      "2023-01-20 15:30:11 | INFO | hw5_transformer | example reference: 他們正在習知我們的文化、語言、傳統 , 並意識到彼此不同及相同之處 。\n",
      "2023-01-20 15:30:11 | INFO | hw5_transformer | validation loss:\t3.4204\n",
      "2023-01-20 15:30:11 | INFO | hw5_transformer | BLEU = 24.31 57.9/32.3/19.0/11.7 (BP = 0.957 ratio = 0.958 hyp_len = 107154 ref_len = 111811)\n",
      "2023-01-20 15:30:11 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint29.pt\n",
      "2023-01-20 15:30:11 | INFO | hw5_transformer | end of epoch 29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4efe10781241909689ad720c8d5eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 30:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:39:36 | INFO | hw5_transformer | training loss: 3.4197\n",
      "2023-01-20 15:39:36 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51023d779bd946c69a6d9d0affbc40ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:40:10 | INFO | hw5_transformer | example source: but did you know that only two percent of that funding went to rebuild haitian public institutions , including its health sector ?\n",
      "2023-01-20 15:40:10 | INFO | hw5_transformer | example hypothesis: 但你是否知道只有百分之二的資金去重建海地公共機構 , 包括它的健康部門 ?\n",
      "2023-01-20 15:40:10 | INFO | hw5_transformer | example reference: 但你知不知道國際社會捐款中只有2%用去重建海地的公共體制 , 包括衞生部門 ?\n",
      "2023-01-20 15:40:10 | INFO | hw5_transformer | validation loss:\t3.4244\n",
      "2023-01-20 15:40:10 | INFO | hw5_transformer | BLEU = 23.93 58.5/32.6/19.1/11.7 (BP = 0.937 ratio = 0.939 hyp_len = 104987 ref_len = 111811)\n",
      "2023-01-20 15:40:11 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint30.pt\n",
      "2023-01-20 15:40:11 | INFO | hw5_transformer | end of epoch 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699a398b450048d4a5346168c2f0bc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 31:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:49:37 | INFO | hw5_transformer | training loss: 3.4113\n",
      "2023-01-20 15:49:37 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dabf7abbbd94fd088e8fdf7160a18bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:50:13 | INFO | hw5_transformer | example source: thank you .\n",
      "2023-01-20 15:50:13 | INFO | hw5_transformer | example hypothesis: 謝謝\n",
      "2023-01-20 15:50:13 | INFO | hw5_transformer | example reference: 謝謝 。\n",
      "2023-01-20 15:50:13 | INFO | hw5_transformer | validation loss:\t3.4159\n",
      "2023-01-20 15:50:13 | INFO | hw5_transformer | BLEU = 24.41 58.1/32.5/19.1/11.8 (BP = 0.956 ratio = 0.957 hyp_len = 106968 ref_len = 111811)\n",
      "2023-01-20 15:50:13 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint31.pt\n",
      "2023-01-20 15:50:13 | INFO | hw5_transformer | end of epoch 31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f883750d5ecd4ca3b268de11049b0619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 32:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:59:37 | INFO | hw5_transformer | training loss: 3.4075\n",
      "2023-01-20 15:59:37 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8babecd2800a4fc2a4f1bdfd5cba7d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 16:00:13 | INFO | hw5_transformer | example source: and i think the answer is a lot of different people are going to have to do a lot of different things .\n",
      "2023-01-20 16:00:13 | INFO | hw5_transformer | example hypothesis: 我想答案是 , 很多不同的人都需要做很多不同的事 。\n",
      "2023-01-20 16:00:13 | INFO | hw5_transformer | example reference: 我的回答是 , 我們所有人各盡其責 。\n",
      "2023-01-20 16:00:13 | INFO | hw5_transformer | validation loss:\t3.4168\n",
      "2023-01-20 16:00:13 | INFO | hw5_transformer | BLEU = 24.46 57.8/32.3/19.0/11.7 (BP = 0.964 ratio = 0.964 hyp_len = 107814 ref_len = 111811)\n",
      "2023-01-20 16:00:13 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint32.pt\n",
      "2023-01-20 16:00:14 | INFO | hw5_transformer | end of epoch 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4048a247104a02861168b8e82a62fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 33:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 16:09:41 | INFO | hw5_transformer | training loss: 3.4007\n",
      "2023-01-20 16:09:41 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddbb75cdad74f8fa291a1cce916fb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 16:10:18 | INFO | hw5_transformer | example source: that ambition that had allowed him to laboriously educate himself by himself , to go through that string of political failures and the darkest days of the war .\n",
      "2023-01-20 16:10:18 | INFO | hw5_transformer | example hypothesis: 那種野心讓他自己有力地教育自己 , 去經歷那一連串的政治失敗 , 以及戰爭最黑暗的日子 。\n",
      "2023-01-20 16:10:18 | INFO | hw5_transformer | example reference: 這樣的企圖心策勵他辛辛苦苦不懈的自我教育 , 鼓舞著他面對一連串的政壇失意及堅強走過戰爭時最黑暗的歲月.\n",
      "2023-01-20 16:10:18 | INFO | hw5_transformer | validation loss:\t3.4010\n",
      "2023-01-20 16:10:18 | INFO | hw5_transformer | BLEU = 24.29 59.0/33.2/19.6/12.1 (BP = 0.929 ratio = 0.932 hyp_len = 104154 ref_len = 111811)\n",
      "2023-01-20 16:10:18 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint33.pt\n",
      "2023-01-20 16:10:18 | INFO | hw5_transformer | end of epoch 33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd19b10ae7643d8903d18c39eec5900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 34:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 16:19:41 | INFO | hw5_transformer | training loss: 3.3949\n",
      "2023-01-20 16:19:41 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106790437f434e028b0ca463d182de06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 16:20:17 | INFO | hw5_transformer | example source: this response has evolved from the physiological mechanism designed to make sure we can survive in a crisis .\n",
      "2023-01-20 16:20:17 | INFO | hw5_transformer | example hypothesis: 這種反應已經從生理機制演化出來 , 來確保我們能在危機中生存下來 。\n",
      "2023-01-20 16:20:17 | INFO | hw5_transformer | example reference: 這是心理機制進化的反應 , 目的是使我們能夠應付危機 , 生存下來 。\n",
      "2023-01-20 16:20:17 | INFO | hw5_transformer | validation loss:\t3.4030\n",
      "2023-01-20 16:20:17 | INFO | hw5_transformer | BLEU = 24.60 57.7/32.2/18.9/11.7 (BP = 0.971 ratio = 0.971 hyp_len = 108569 ref_len = 111811)\n",
      "2023-01-20 16:20:17 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint34.pt\n",
      "2023-01-20 16:20:17 | INFO | hw5_transformer | end of epoch 34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f788ad261bf54b849be1eadcb09e24bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 35:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 16:29:41 | INFO | hw5_transformer | training loss: 3.3895\n",
      "2023-01-20 16:29:41 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40837d830e94875a543b68dc7cca11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 16:30:18 | INFO | hw5_transformer | example source: see that chair there ? the orange one ? that's the organic chair . 1940 .\n",
      "2023-01-20 16:30:18 | INFO | hw5_transformer | example hypothesis: 看到這張椅子嗎 ? 橘色一號 ? 這是1940年的有機椅子 。\n",
      "2023-01-20 16:30:18 | INFO | hw5_transformer | example reference: 看見那張椅子嗎 ? 橘黃色那個 ? 這是有機椅子 。 一九四零年 。\n",
      "2023-01-20 16:30:18 | INFO | hw5_transformer | validation loss:\t3.3953\n",
      "2023-01-20 16:30:18 | INFO | hw5_transformer | BLEU = 24.74 57.5/32.2/18.9/11.7 (BP = 0.979 ratio = 0.980 hyp_len = 109522 ref_len = 111811)\n",
      "2023-01-20 16:30:19 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint35.pt\n",
      "2023-01-20 16:30:19 | INFO | hw5_transformer | end of epoch 35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfdddb07dbd4482b5dfd0c04de9edac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 36:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 16:39:40 | INFO | hw5_transformer | training loss: 3.3845\n",
      "2023-01-20 16:39:40 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58501dfe0014ec3892f8781bec14234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 16:40:15 | INFO | hw5_transformer | example source: do they go from one side of the pacific to the other ?\n",
      "2023-01-20 16:40:15 | INFO | hw5_transformer | example hypothesis: 他們是否從太平洋的一邊走到另一邊 ?\n",
      "2023-01-20 16:40:15 | INFO | hw5_transformer | example reference: 牠們會從太平洋的一端游到另一端嗎 ?\n",
      "2023-01-20 16:40:15 | INFO | hw5_transformer | validation loss:\t3.3922\n",
      "2023-01-20 16:40:15 | INFO | hw5_transformer | BLEU = 24.53 58.1/32.6/19.2/11.9 (BP = 0.957 ratio = 0.958 hyp_len = 107132 ref_len = 111811)\n",
      "2023-01-20 16:40:15 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint36.pt\n",
      "2023-01-20 16:40:15 | INFO | hw5_transformer | end of epoch 36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8101b849251b4e20a3d72b7787901a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 37:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 16:49:36 | INFO | hw5_transformer | training loss: 3.3764\n",
      "2023-01-20 16:49:36 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70487f3dcb34895a6a31a5889cb4bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 16:50:12 | INFO | hw5_transformer | example source: i do , in any case .\n",
      "2023-01-20 16:50:12 | INFO | hw5_transformer | example hypothesis: 無論如何 , 我都做 。\n",
      "2023-01-20 16:50:12 | INFO | hw5_transformer | example reference: 無論如何我也認同 。\n",
      "2023-01-20 16:50:12 | INFO | hw5_transformer | validation loss:\t3.3961\n",
      "2023-01-20 16:50:12 | INFO | hw5_transformer | BLEU = 24.49 58.7/33.1/19.5/12.1 (BP = 0.942 ratio = 0.943 hyp_len = 105485 ref_len = 111811)\n",
      "2023-01-20 16:50:12 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint37.pt\n",
      "2023-01-20 16:50:12 | INFO | hw5_transformer | end of epoch 37\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6107f6c5244646b109ab90977a9553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 38:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 16:59:43 | INFO | hw5_transformer | training loss: 3.3700\n",
      "2023-01-20 16:59:43 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed45f7dd5c94fc1b389bf693844c3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 17:00:19 | INFO | hw5_transformer | example source: and as you think about that experience , i want you to ask yourself: how might that experience of being a privileged player in a rigged game change the way you think about yourself and regard that other player ?\n",
      "2023-01-20 17:00:19 | INFO | hw5_transformer | example hypothesis: 當你想到這個經驗時 , 我希望你們自問:身為一個有特權的玩家的經驗會如何改變你對自己的想法 , 並認同其他玩家的看法 ?\n",
      "2023-01-20 17:00:19 | INFO | hw5_transformer | example reference: 當你這麼想像時也請自問在這被動手腳的遊戲中作為佔上風的玩家會如何改變對自己和其他玩家的看法呢 ?\n",
      "2023-01-20 17:00:19 | INFO | hw5_transformer | validation loss:\t3.3994\n",
      "2023-01-20 17:00:19 | INFO | hw5_transformer | BLEU = 24.35 58.6/32.9/19.3/11.9 (BP = 0.944 ratio = 0.945 hyp_len = 105682 ref_len = 111811)\n",
      "2023-01-20 17:00:19 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint38.pt\n",
      "2023-01-20 17:00:19 | INFO | hw5_transformer | end of epoch 38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b411210ba86a4cd7ae0733b62c1e49be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 39:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 17:09:42 | INFO | hw5_transformer | training loss: 3.3673\n",
      "2023-01-20 17:09:42 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d483ea14444644dba96421432b0e9f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 17:10:21 | INFO | hw5_transformer | example source: this is a photograph i took at the northern tip of baffin island when i went narwhal hunting with some inuit people , and this man , olayuk , told me a marvelous story of his grandfather .\n",
      "2023-01-20 17:10:21 | INFO | hw5_transformer | example hypothesis: 這是一張我在巴芬島北部拍的照片 , 當我和一些英屬人打獵時 , 這位男子 , 歐拉尤克告訴我他祖父的故事 。\n",
      "2023-01-20 17:10:21 | INFO | hw5_transformer | example reference: 這張照片是我在巴芬島的北邊山頂上拍攝的那時我和一些因紐特人去捕獨角鯨這個叫奧拉雅的人述說了他祖父的傳奇故事\n",
      "2023-01-20 17:10:21 | INFO | hw5_transformer | validation loss:\t3.3906\n",
      "2023-01-20 17:10:21 | INFO | hw5_transformer | BLEU = 24.41 58.7/33.2/19.7/12.3 (BP = 0.931 ratio = 0.934 hyp_len = 104401 ref_len = 111811)\n",
      "2023-01-20 17:10:21 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint39.pt\n",
      "2023-01-20 17:10:21 | INFO | hw5_transformer | end of epoch 39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409fb10e406a4c01a0874febce86b910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train epoch 40:   0%|          | 0/791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 17:19:41 | INFO | hw5_transformer | training loss: 3.3631\n",
      "2023-01-20 17:19:41 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e74e58b556428f9273b61d8b755a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 17:20:16 | INFO | hw5_transformer | example source: congratulations .\n",
      "2023-01-20 17:20:16 | INFO | hw5_transformer | example hypothesis: 恭喜 。\n",
      "2023-01-20 17:20:16 | INFO | hw5_transformer | example reference: 恭喜各位 。\n",
      "2023-01-20 17:20:16 | INFO | hw5_transformer | validation loss:\t3.3878\n",
      "2023-01-20 17:20:16 | INFO | hw5_transformer | BLEU = 24.57 58.8/33.2/19.6/12.1 (BP = 0.942 ratio = 0.943 hyp_len = 105485 ref_len = 111811)\n",
      "2023-01-20 17:20:16 | INFO | hw5_transformer | saved epoch checkpoint: /Data/sda/ruizhe/Lee_ML2022_HW/HW05_Transformer/checkpoints/transformer/checkpoint40.pt\n",
      "2023-01-20 17:20:16 | INFO | hw5_transformer | end of epoch 40\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device=device)\n",
    "criterion = criterion.to(device=device)\n",
    "     \n",
    "\n",
    "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
    "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
    "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
    "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
    "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
    "logger.info(\n",
    "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
    "        sum(p.numel() for p in model.parameters()),\n",
    "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "    )\n",
    ")\n",
    "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")\n",
    "     \n",
    "\n",
    "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
    "try_load_checkpoint(model, optimizer, name=config.resume)\n",
    "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
    "    # train for one epoch\n",
    "    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
    "    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
    "    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
    "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(checkpoint_upper_bound=None, inputs=['./checkpoints/transformer'], num_epoch_checkpoints=5, num_update_checkpoints=None, output='./checkpoints/transformer/avg_last_5_checkpoint.pt')\n",
      "averaging checkpoints:  ['./checkpoints/transformer/checkpoint40.pt', './checkpoints/transformer/checkpoint39.pt', './checkpoints/transformer/checkpoint38.pt', './checkpoints/transformer/checkpoint37.pt', './checkpoints/transformer/checkpoint36.pt']\n",
      "Finished writing averaged checkpoint to ./checkpoints/transformer/avg_last_5_checkpoint.pt\n",
      "2023-01-20 17:29:33 | INFO | hw5_transformer | loaded checkpoint checkpoints/transformer/avg_last_5_checkpoint.pt: step=unknown loss=3.387755870819092 bleu=24.568838374807136\n",
      "2023-01-20 17:29:33 | INFO | hw5_transformer | begin validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376e999e3b4d4b13baf7e5fcf485a147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 17:30:07 | INFO | hw5_transformer | example source: and finally , my most recent image , which has such a special meaning for me personally: this is the serengeti national park in tanzania .\n",
      "2023-01-20 17:30:07 | INFO | hw5_transformer | example hypothesis: 最後 , 我最近的影像 , 對我個人來說 , 有一個特別的意義:這是坦尚尼亞的塞倫蒂國家公園 。\n",
      "2023-01-20 17:30:07 | INFO | hw5_transformer | example reference: 最後 , 我最新的影像 , 對我個人有特別的意義:這是位於坦尚尼亞的塞倫蓋提國家公園 。\n",
      "2023-01-20 17:30:07 | INFO | hw5_transformer | validation loss:\t3.3736\n",
      "2023-01-20 17:30:07 | INFO | hw5_transformer | BLEU = 24.76 58.7/33.2/19.7/12.2 (BP = 0.945 ratio = 0.947 hyp_len = 105875 ref_len = 111811)\n",
      "2023-01-20 17:30:07 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/data-bin/ted2020/test.en-zh.en\n",
      "2023-01-20 17:30:07 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/data-bin/ted2020/test.en-zh.zh\n",
      "2023-01-20 17:30:07 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 test en-zh 4000 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b930df820cd42b2b2347c7b2b591826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prediction:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# averaging a few checkpoints can have a similar effect to ensemble\n",
    "checkdir=config.savedir\n",
    "!python ./fairseq/scripts/average_checkpoints.py \\\n",
    "--inputs {checkdir} \\\n",
    "--num-epoch-checkpoints 5 \\\n",
    "--output {checkdir}/avg_last_5_checkpoint.pt\n",
    "     \n",
    "# Confirm model weights used to generate submission\n",
    "# checkpoint_last.pt : latest epoch\n",
    "# checkpoint_best.pt : highest validation bleu\n",
    "# avg_last_5_checkpoint.pt:　the average of last 5 epochs\n",
    "try_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")\n",
    "validate(model, task, criterion, log_to_wandb=False)\n",
    "None\n",
    "     \n",
    "# Generate Prediction\n",
    "def generate_prediction(model, task, split=\"test\", outfile=\"./prediction.txt\"):    \n",
    "    task.load_dataset(split=split, epoch=1)\n",
    "    itr = load_data_iterator(task, split, 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
    "    \n",
    "    idxs = []\n",
    "    hyps = []\n",
    "\n",
    "    model.eval()\n",
    "    progress = tqdm.tqdm(itr, desc=f\"prediction\")\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(progress):\n",
    "            # validation loss\n",
    "            sample = utils.move_to_cuda(sample, device=device)\n",
    "\n",
    "            # do inference\n",
    "            s, h, r = inference_step(sample, model)\n",
    "            \n",
    "            hyps.extend(h)\n",
    "            idxs.extend(list(sample['id']))\n",
    "            \n",
    "    # sort based on the order before preprocess\n",
    "    hyps = [x for _,x in sorted(zip(idxs,hyps))]\n",
    "    \n",
    "    with open(outfile, \"w\") as f:\n",
    "        for h in hyps:\n",
    "            f.write(h+\"\\n\")\n",
    "\n",
    "generate_prediction(model, task)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-zh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcf2d9ec195e4d201fa2eee03e196b15dfc39361815a278fddc720386dc92bb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
